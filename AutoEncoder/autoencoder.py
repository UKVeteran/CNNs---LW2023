# -*- coding: utf-8 -*-
"""autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBUusZplo7g7JsKS9NBB0xz8zRfluUf_

# Autoencoders

üéØ **Exercise objectives**
- Discover ***autoencoders***
- Get a deeper understanding of CNNs

<hr>

üëâ There exists a very particular architecture in Deep Learning called **`Autoencoders`**. Autoencoders are Neural Network architectures trained to return **outputs that are as similar as possible to the original inputs fed to them**. Why would we do that?  

Before answering the question _"why"_, let's answer the question _"how"_.

üë©üèª‚Äçüè´ <u>***How does an autoencoder work ?***</u>

There are two parts in an autoencoder: the  **`encoder`** and the **`decoder`**.

1. In the encoder, we will make the information flow through different dense layers with a decreasing number of neurons. It will create a **`bottleneck`** where the information is compressed.

2. In the decoder, we will try to recreate the original data based on the compressed data.

üî• <u>***Why is it powerful or useful?***</u>

If it works well, it means two important things:

* ‚úÖ We can afford to **compress our dataset** and use a compressed version of it when fitting another Neural Network!

* ‚úÖ The **information contained in the bottleneck** - i.e. the data compressed in a low-dimensional layer - **accurately captures the patterns of our dataset** and the autoencoder is able to decode the compressed information!

üå† <u>**Applications:**</u>
- Image compression
- Denoising (cf. Google Pixel phones...)
- Image generation!


<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/autoencoder.png'>

## Google Colab Setup

Repeat the same process from the last challenge to upload your challenge folder and open your notebook:

1. access your [Google Drive](https://drive.google.com/)
2. go into the Colab Notebooks folder
3. drag and drop this challenge's folder into it
4. right-click the notebook file and select `Open with` $\rightarrow$ `Google Colaboratory`

Don't forget to enable GPU acceleration!

`Runtime` $\rightarrow$ `Change runtime type` $\rightarrow$ `Hardware accelerator` $\rightarrow$ `GPU`

When this is done, run the cells below and get to work!
"""

# Mount GDrive
from google.colab import drive
drive.mount('/content/drive')

# Put Colab in the context of this challenge
import os

# os.chdir allows you to change directories, like cd in the Terminal
os.chdir('/content/drive/MyDrive/Colab Notebooks/')

"""You are now good to go, proceed with the challenge! Don't forget to copy everything back to your PC to upload to Kitt üöÄ

## (0) The MNIST Dataset

In this notebook, we will train an auto-encoder to work on 28x28 grey images from the MNIST dataset, available in Keras. Run the cells below
"""

from tensorflow.keras.datasets import mnist

(images_train, labels_train), (images_test, labels_test) = mnist.load_data()
print(images_train.shape)
print(images_test.shape)

# Add a channels for the colors and normalize data
X_train = images_train.reshape((60000, 28, 28, 1)) / 255.
X_test = images_test.reshape((10000, 28, 28, 1)) / 255.

# Plot some images
import matplotlib.pyplot as plt

f, axs = plt.subplots(1, 10, figsize=(20, 4))
for i, ax in enumerate(axs):
    ax.axis('off')
    ax.imshow(X_train[i].reshape(28, 28), cmap='Greys')

plt.show()

"""## (1) The encoder

üéÅ First, we built the "Encoder" part for you.

üëâ  Notice how similar it looks compared to a Convolutional Classifier with **latent_dimension** neurons at the end. However, we using the "tanh" activation function in the final dense layer instead of "relu".
"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

def build_encoder(latent_dimension):
    '''returns an encoder model, of output_shape equals to latent_dimension'''
    encoder = Sequential()

    encoder.add(Conv2D(8, (2,2), input_shape=(28, 28, 1), activation='relu'))
    encoder.add(MaxPooling2D(2))

    encoder.add(Conv2D(16, (2, 2), activation='relu'))
    encoder.add(MaxPooling2D(2))

    encoder.add(Conv2D(32, (2, 2), activation='relu'))
    encoder.add(MaxPooling2D(2))

    encoder.add(Flatten())
    encoder.add(Dense(latent_dimension, activation='tanh'))

    return encoder

"""‚ùì **Question: building an encoder** ‚ùì

Build your encoder with **`latent_dimension = 2`** and look at the number of parameters.
"""

encoder = build_encoder(2)
encoder.summary()

"""## (2) Decoder

It's your turn to build the decoder this time!

We need to build a üî• **`reversed CNN` üî•** that
* takes a dense layer as input,
* and outputs an image of shape $ (28,28,1) $ similar to our MNIST images.

üìö For this purpose, we will use a new layer called <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">**`Conv2DTranspose`**</a> üìö
    
The name of this layer speaks for itself: it performs the opposite of a convolution operation!

üí° We will follow this strategy:
* Start by reshaping the Dense Input Layer into an Image of shape $(7,7,..)$
* Then apply the `Conv2DTranspose` operation with ***strides = 2*** to double the output shape to $(14,14,..)$
* then add another Conv2DTranpose layer on top of the first one to make it $(28,28,1)$.

<hr>

‚ùì **Question: the architecture of a decoder** ‚ùì


Define a **`decoding architecture`** in the method below as follows:
- a *Dense* layer with:
    - $7 \times 7 \times 8$ neurons,
    - *input_shape* = (latent_dimension, )
    - *tanh* activation function.
- a *Reshape* layer that reshapes to $(7, 7, 8)$ tensors
- a *Conv2DTranspose* with:
    - $8$ filters,
    - $(2,2)$ kernels,
    - strides of $2$,
    - padding *same*
    - _relu_ activation function
- a second Conv2DTranspose layer with:
    - $1$ filter,
    - $(2,2)$ kernels,
    - strides of $2$,
    - padding _same_,
    - _relu_ activation function
"""

from tensorflow.keras.layers import Reshape, Conv2DTranspose

def build_decoder(latent_dimension):

    decoder = Sequential()

    decoder.add(Dense(7*7*8, activation='tanh', input_shape=(latent_dimension,)))
    decoder.add(Reshape((7, 7, 8)))  # no batch axis here
    decoder.add(Conv2DTranspose(8, (2, 2), strides=2, padding='same', activation='relu'))

    decoder.add(Conv2DTranspose(1, (2, 2), strides=2, padding='same', activation='relu'))
    return decoder

"""‚ùì **Question: buiding a decoder** ‚ùì

Build your decoder with **`latent_dimension = 2`** and check that it outputs images of same shape than the encoder input
"""

decoder = build_decoder(2)
decoder.summary()

"""## (3) Auto-Encoder

üéâ We can now **concatenate** both **`the encoder and the decoder`** thanks to the **`Model`** class in Keras, using the **`Functional API`**.
"""

from tensorflow.keras import Model
from tensorflow.keras.layers import Input

def build_autoencoder(encoder, decoder):
    inp = Input((28, 28,1))
    encoded = encoder(inp)
    decoded = decoder(encoded)
    autoencoder = Model(inp, decoded)
    return autoencoder

"""‚ùì **Questions** ‚ùì

* Try to understand syntax above üëÜ
* Build your autoencoder
* Have a look at the number of parameters
"""

autoencoder = build_autoencoder(encoder, decoder)
autoencoder.summary()

"""‚ùì **Question: Compiling an autoencoder** ‚ùì

Define a method which compiles your model. Pick an appropriate loss.

<u><i>Think carefully:</i></u> ü§î On which mathematical object are we going to compare *predictions* and the *ground truth* for the computation of the loss function and the metrics?


<details>
    <summary><i>Answer</i></summary>

It should compare two images (Black and White in our case), pixel-by-pixel!
    
The MSE loss seems to be an appropriate loss function for pixel-by-pixel error minimization.
</details>
"""

def compile_autoencoder(autoencoder):
    autoencoder.compile(loss='mse',
                  optimizer='adam')

"""‚ùì **Question: Training an autoencoder** ‚ùì  

* Compile your model and fit it with `batch_size = 32` and `epochs = 20`.
* What is the label `y_train` in this case?

<i>Note:</i> Don't waste your time fighting overfitting in this challenge, you will have time to care about this during the project weeks :)
"""

compile_autoencoder(autoencoder)
autoencoder.fit(X_train, X_train, epochs = 20, batch_size = 32)

prediction = autoencoder.predict(X_train, verbose=0, batch_size=100)# you can now display an image to see it is reconstructed well

for i in range(3):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4,2))
    ax1.imshow(prediction[i].reshape(28,28), cmap = 'Greys')
    ax2.imshow(X_train[i].reshape(28,28), cmap = 'Greys')
    plt.show()

"""‚ùì **Question: Encoding the dataset** ‚ùì

* Using only the encoder part of the network, encode your dataset and save it under `X_encoded` .
    * Each image is now represented by two values (that correspond to the dimension of the latent space, of the bottleneck; aka the `latent_dimension`.
"""

X_encoded = encoder.predict(X_train, verbose=1)

"""ü§î Where are we after running the encoder?

* Each image was compressed into a 2D space.
* Each of these handwritten digit have a given label, between 0 and 9, but the goal here is not to classify these pictures like in the first challenge but to **reconstruct the original image before the compression**.

‚ùì **Question: Visualizing handwritten digits in the latent space** ‚ùì

Scatterplot the encoded data (only a small fraction of the encoded dataset for visibility purposes...)
- Each point of the scatter plot  corresponds to an encoded image
- Color the dots according to their respective labels (digit representation):
    - for instance, all the "4"s should be represented by a color on this scatter plot...
    - ...while the "5" should be represented by another color
    - choose a set of [`qualitative colormaps`](https://matplotlib.org/stable/gallery/color/colormap_reference.html)

What do you remark about this plot?
"""

labels_train[:300]

import seaborn as sns
plt.scatter(x = X_encoded[:300, 0],
            y = X_encoded[:300, 1],
            c = labels_train[:300],
            cmap = 'Set1');

"""## (4) Application: Image generation

‚ùì **Questions: Generate new digits** ‚ùì

* Let's create some new digits!
* Run the following code editing the latent coordinates.
* Play with the coordinates start with ones from the graph above.
* For example [0.75, 0.75] is the "zero" area with latent space (don't forget to experiment outside the boundaries of our original dataset)
"""

import numpy as np

latent_coords = np.array([[0.75, -0.75]])
generated_img = decoder.predict(latent_coords)
plt.imshow(generated_img.reshape(28,28), cmap='Greys')

"""‚ùóÔ∏è We can reuse the decoder of auto encoders as one of the simplest forms of generative deep learning. When we enter coordinates that were not in the training dataset **we are creating never seen before digits!**

## (5) Application: Image denoising

‚ùì **Questions: Creating some noise in the dataset** ‚ùì

* Let's add some noise to the input data.
* Run the following code
* Plot some handwritten digits and their noisy versions
"""

import numpy as np

noise_factor = 0.5

X_train_noisy = X_train + noise_factor * np.random.normal(0., 1., size=X_train.shape)
X_test_noisy = X_test + noise_factor * np.random.normal(0., 1., size=X_test.shape)

for i in range(3):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4,2))
    ax1.imshow(X_train[i].reshape(28,28), cmap='Greys')
    ax2.imshow(X_train_noisy[i].reshape(28,28), cmap='Greys')
    plt.show()

"""‚ùì **Question: decoding the noisy pictures** ‚ùì

* Reinitialize your autoencoder (with a latent space of 2)
* Train it again, this time using the noisy train dataset instead of the normal train dataset
    * *Keep `batch_size = 32` and `epochs = 5`*
* What do you expect if you run the autoencoder on the noisy data instead of the original data in terms of performance?
"""

encoder = build_encoder(2)
decoder = build_decoder(2)
autoencoder = build_autoencoder(encoder, decoder)
compile_autoencoder(autoencoder)

history_denoising = autoencoder.fit(X_train_noisy, X_train,
                                    epochs = 20,
                                    batch_size=32)

"""‚ùì **Question: comparing the noisy test images with the denoised images** ‚ùì

For some noisy test images, predict the denoised images and plot the results side by side...
"""

prediction = autoencoder.predict(X_test_noisy, verbose=1)

for i in range(10):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4,2))
    ax1.imshow(prediction[i].reshape(28,28), cmap='Greys')
    ax2.imshow(X_test_noisy[i].reshape(28,28), cmap='Greys')
    plt.show()

"""‚ùì **Question: choosing the "correct" latent_dimension** ‚ùì

Now, try to evaluate which **`latent_dimension`** is the most suitable in order to have **`the best image reconstruction preprocess`** $ \Leftrightarrow $ How to remove as much noise as possible in the noisy dataset using the latent dimension?`
"""

latent_dimensions = list(range(2,20,3))

test_errors = []
for latent_dimension in latent_dimensions:
    print(" ")
    print("-"*80)
    print(f"Running the autoencoder with latent_dimension = {latent_dimension}")
    print("-"*80)
    encoder = build_encoder(latent_dimension=latent_dimension)
    decoder = build_decoder(latent_dimension=latent_dimension)
    autoencoder = build_autoencoder(encoder, decoder)
    compile_autoencoder(autoencoder)
    autoencoder.fit(X_train, X_train, epochs=20, batch_size=32)
    error = autoencoder.evaluate(X_test, X_test)
    test_errors.append(error)

import matplotlib.pyplot as plt

with plt.style.context('seaborn-deep'):
    # figsize
    plt.figure(figsize=(10,6))
    # getting axes
    ax = plt.gca()
    # plotting
    ax.plot(latent_dimensions,
            test_errors,
            color='black',
            linestyle='dashed',
            marker='o',
            markerfacecolor='#947FFF',
            markersize=10)
    # more
    ax.set_title('MSE Test Error vs. $Latent$ $Dimensions$', fontsize = 18)
    ax.set_xlabel('Latent Dimension', fontsize = 14)
    ax.set_ylabel('MSE Score', fontsize = 14)
    ax.grid(axis="x",linewidth=0.5)
    ax.grid(axis="y",linewidth=0.5)

    # focusing
    ax.scatter(8,test_errors[2],c='#00ad8b',s=700)

    # annotate
    ax.annotate("Elbow Method ?",
               xy=(8,test_errors[2]),
                xytext=(8+0.25,test_errors[2]+0.0025),
                arrowprops=dict(arrowstyle='-|>',
                                fc="k",
                                ec="k",
                                lw=2),
                bbox=dict(pad=5, facecolor="none", edgecolor="none")
               )


    plt.show();

"""ü•° <b><u>Conclusion</u></b>


* It is obvious that:
    * if you compress your pictures of size $ 28 \times 28 $ into a 1D space, you will lose a ton of information.
    * if you compress them into a $ 28 \times 28 = 784$ -space, you are actually not compressing them
    
* We can still use this graph of **Loss vs. Latent dimensions** reading it from right to left to decide in which latent space it would be advisable to compress the pictures without losing to much information: `latent_space = 8` seems a sweat spot here using the Elbow Method.

---

üèÅ **Congratulations** üèÅ

1. Download this notebook from your `Google Drive` or directly from `Google Colab`
2. Drag-and-drop it from your `Downloads` folder to your local challenge folder


üíæ Don't forget to push your code

3. Follow the usual procedure on your terminal inside the challenge folder:
      * *git add autoencoder.ipynb*
      * *git commit -m "I am the god of Transfer Learning"*
      * *git push origin master*

*Hint*: To find where this Colab notebook has been saved, click on `File` $\rightarrow$ `Locate in Drive`.

üòâ That was the last challenge of this module!
"""